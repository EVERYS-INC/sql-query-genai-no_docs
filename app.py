import streamlit as st
import pandas as pd
from dotenv import load_dotenv
import os
from src.database import DatabaseConnection
from src.openai_client import OpenAIClient
from src.visualizer import DataVisualizer

load_dotenv()

st.set_page_config(
    page_title="è£½é€ æ¥­å·¥å ´ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ­",
    layout="wide"
)

def main():
    st.title("ğŸ­ è£½é€ æ¥­å·¥å ´ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("è‡ªç„¶è¨€èªã§å·¥å ´ã®ç”Ÿç”£ãƒ‡ãƒ¼ã‚¿ã€æ©Ÿå™¨ç¨¼åƒçŠ¶æ³ã€å“è³ªæƒ…å ±ã‚’åˆ†æã—ã¾ã™")
    
    # åˆæœŸåŒ–æ™‚ã«è‡ªå‹•çš„ã«SQLiteã«æ¥ç¶š
    if 'db_connection' not in st.session_state:
        try:
            st.session_state.db_connection = DatabaseConnection(
                db_type="sqlite",
                database="database/factory_data.db"
            )
        except Exception as e:
            st.session_state.db_connection = None
            st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è‡ªå‹•æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    if 'openai_client' not in st.session_state:
        try:
            st.session_state.openai_client = OpenAIClient()
        except Exception as e:
            st.session_state.openai_client = None
            st.error(f"Azure OpenAIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    if 'llama_client' not in st.session_state:
        st.session_state.llama_client = None
    
    if 'gemma_client' not in st.session_state:
        st.session_state.gemma_client = None
    
    if 'llama_device_mode' not in st.session_state:
        st.session_state.llama_device_mode = 'auto'  # 'auto', 'gpu', 'cpu'
    
    with st.sidebar:
        st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šçŠ¶æ…‹ã®è¡¨ç¤º
        st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š")
        if st.session_state.db_connection:
            st.success("âœ… SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæ¸ˆã¿")
            st.info("ğŸ“ database/factory_data.db")
        else:
            st.error("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœªæ¥ç¶š")
        
        # AIãƒ¢ãƒ‡ãƒ«é¸æŠ
        st.subheader("ğŸ¤– AIãƒ¢ãƒ‡ãƒ«é¸æŠ")
        if 'selected_model' not in st.session_state:
            st.session_state.selected_model = 'azure_openai'
        
        st.session_state.selected_model = st.radio(
            "ä½¿ç”¨ã™ã‚‹AIãƒ¢ãƒ‡ãƒ«",
            options=['azure_openai', 'llama_elyza', 'gemma_2b'],
            format_func=lambda x: {
                'azure_openai': "â˜ï¸ Azure OpenAI (é«˜é€Ÿãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰)",
                'llama_elyza': "ğŸ¦™ Llama-3-ELYZA-JP-8B (16GB)",
                'gemma_2b': "ğŸ’ Gemma-2-2b-jpn-it (è»½é‡ãƒ»é«˜é€Ÿ)"
            }[x],
            key="model_selector",
            help="Azure OpenAI: ã‚¯ãƒ©ã‚¦ãƒ‰æ¥ç¶šå¿…é ˆ | Llama-3: é«˜ç²¾åº¦ãƒ»16GB | Gemma-2: è»½é‡ãƒ»4GB VRAMå¯¾å¿œ"
        )
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®æ¥ç¶šçŠ¶æ…‹è¡¨ç¤º
        if st.session_state.selected_model == 'azure_openai':
            if st.session_state.openai_client:
                st.success("âœ… Azure OpenAIæ¥ç¶šæ¸ˆã¿")
            else:
                st.error("âŒ Azure OpenAIæœªæ¥ç¶š")
        elif st.session_state.selected_model == 'gemma_2b':
            if st.session_state.gemma_client:
                st.success("âœ… Gemma-2-2b-jpn-itèª­ã¿è¾¼ã¿æ¸ˆã¿")
                current_device = "GPU" if hasattr(st.session_state.gemma_client, 'device') and st.session_state.gemma_client.device == "cuda" else "CPU"
                st.info(f"ğŸ’ ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰: {current_device} (è»½é‡ãƒ¢ãƒ‡ãƒ«)")
                if st.button("ãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿"):
                    st.session_state.gemma_client = None
                    st.rerun()
            else:
                # GPUåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                import torch
                gpu_available = torch.cuda.is_available()
                
                if gpu_available:
                    st.info(f"âš¡ GPUãŒåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
                    st.success("ğŸ‰ Gemma-2ã¯4GB VRAMã§ã‚‚å¿«é©ã«å‹•ä½œï¼")
                else:
                    st.info("ğŸ’» CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ï¼ˆGemmaã¯è»½é‡ãªã®ã§CPUã§ã‚‚é«˜é€Ÿï¼‰")
                
                if st.button("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€", type="primary"):
                    with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                        try:
                            from src.gemma_client import GemmaClient
                            # 4GB VRAMã®å ´åˆã¯å¼·åˆ¶çš„ã«CPUãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                            force_cpu = not gpu_available or (gpu_available and torch.cuda.get_device_properties(0).total_memory < 5 * 1024**3)
                            st.session_state.gemma_client = GemmaClient(force_cpu=force_cpu)
                            st.success("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
                            st.rerun()
                        except Exception as e:
                            st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                else:
                    st.warning("âš ï¸ Gemmaãƒ¢ãƒ‡ãƒ«æœªèª­ã¿è¾¼ã¿")
                    st.info("ä¸Šã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„")
        else:  # llama_elyza
            if st.session_state.llama_client:
                st.success("âœ… Llama-3-ELYZA-JP-8Bèª­ã¿è¾¼ã¿æ¸ˆã¿")
                current_device = "GPU" if hasattr(st.session_state.llama_client, 'device') and st.session_state.llama_client.device == "cuda" else "CPU"
                st.info(f"ğŸ’» ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰: {current_device}")
                if st.button("ãƒ¢ãƒ‡ãƒ«ã‚’å†èª­ã¿è¾¼ã¿"):
                    st.session_state.llama_client = None
                    st.rerun()
            else:
                # GPUåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                import torch
                gpu_available = torch.cuda.is_available()
                
                if gpu_available:
                    st.info(f"âš¡ GPUãŒåˆ©ç”¨å¯èƒ½ã§ã™: {torch.cuda.get_device_name(0)}")
                else:
                    st.info("ğŸ’» GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
                
                # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
                device_mode = st.radio(
                    "å‹•ä½œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ",
                    options=['auto', 'gpu', 'cpu'],
                    format_func=lambda x: {
                        'auto': 'ğŸ…°ï¸ è‡ªå‹•é¸æŠ (GPUå„ªå…ˆ)',
                        'gpu': 'âš¡ GPUãƒ¢ãƒ¼ãƒ‰ (é«˜é€Ÿ)',
                        'cpu': 'ğŸ’» CPUãƒ¢ãƒ¼ãƒ‰'
                    }[x],
                    key="device_selector",
                    help="GPUãƒ¢ãƒ¼ãƒ‰ã¯é«˜é€Ÿã§ã™ãŒã€NVIDIA GPUãŒå¿…è¦ã§ã™"
                )
                
                if st.button("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€", type="primary"):
                    with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                        try:
                            # ãƒ‡ãƒã‚¤ã‚¹ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
                            if device_mode == 'cpu':
                                from src.llama_client_cpu import LlamaClientCPU
                                st.session_state.llama_client = LlamaClientCPU()
                            elif device_mode == 'gpu':
                                if not gpu_available:
                                    st.error("âš ï¸ GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
                                    from src.llama_client_cpu import LlamaClientCPU
                                    st.session_state.llama_client = LlamaClientCPU()
                                else:
                                    try:
                                        from src.llama_client import LlamaClient
                                        st.session_state.llama_client = LlamaClient(force_cpu=False)
                                    except Exception as gpu_error:
                                        st.warning(f"GPUèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(gpu_error)}")
                                        st.info("CPUãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
                                        from src.llama_client_cpu import LlamaClientCPU
                                        st.session_state.llama_client = LlamaClientCPU()
                            else:  # auto
                                try:
                                    from src.llama_client import LlamaClient
                                    st.session_state.llama_client = LlamaClient(force_cpu=False)
                                except Exception:
                                    st.info("CPUãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ã¾ã™...")
                                    from src.llama_client_cpu import LlamaClientCPU
                                    st.session_state.llama_client = LlamaClientCPU()
                            
                            st.session_state.llama_device_mode = device_mode
                            st.success("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
                            st.rerun()
                        except Exception as e:
                            st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                else:
                    st.warning("âš ï¸ Llamaãƒ¢ãƒ‡ãƒ«æœªèª­ã¿è¾¼ã¿")
                    st.info("ä¸Šã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„")
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
        st.subheader("ğŸ”§ ãƒ‡ãƒãƒƒã‚°è¨­å®š")
        if 'debug_mode' not in st.session_state:
            st.session_state.debug_mode = False
        st.session_state.debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰", value=st.session_state.debug_mode, key="debug_sidebar")
        if st.session_state.debug_mode:
            st.info("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨AIå‡ºåŠ›ã‚’è¡¨ç¤ºã—ã¾ã™")
    
    with st.expander("ğŸ’¡ ã‚¯ã‚¨ãƒªã‚µãƒ³ãƒ—ãƒ«ã‚’è¦‹ã‚‹"):
        st.markdown("""
        **ç”Ÿç”£å®Ÿç¸¾åˆ†æ**
        - ä»Šæœˆã®è£½å“åˆ¥ã®ç”Ÿç”£æ•°ã‚’æ•™ãˆã¦
        - å„ãƒ©ã‚¤ãƒ³ã®ä»Šæœˆã®ç”Ÿç”£åŠ¹ç‡ã‚’æ¯”è¼ƒã—ã¦
        - ä¸è‰¯ç‡ãŒæœ€ã‚‚é«˜ã„è£½å“ãƒˆãƒƒãƒ—5
        
        **æ©Ÿå™¨ç¨¼åƒåˆ†æ**
        - å…¨æ©Ÿå™¨ã®ä»Šæ—¥ã®ç¨¼åƒç‡ã‚’è¦‹ã›ã¦
        - ä»Šé€±ã®æ©Ÿå™¨åœæ­¢æ™‚é–“ãŒé•·ã„é †ã«è¡¨ç¤º
        - å„ãƒ©ã‚¤ãƒ³ã®ä»Šæœˆã®OEEã‚’è¨ˆç®—ã—ã¦
        
        **å“è³ªç®¡ç†**
        - ä»Šé€±ã®å“è³ªæ¤œæŸ»åˆæ ¼ç‡ã‚’è£½å“åˆ¥ã«è¡¨ç¤º
        - ä¸è‰¯ã‚¿ã‚¤ãƒ—åˆ¥ã®ç™ºç”Ÿä»¶æ•°ã‚’é›†è¨ˆ
        
        **ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹**
        - ä»Šæœˆå®Ÿæ–½ã—ãŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä¸€è¦§
        - ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚³ã‚¹ãƒˆãŒé«˜ã„æ©Ÿå™¨ãƒˆãƒƒãƒ—5
        """)
    
    query_input = st.text_area(
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®å•ã„åˆã‚ã›ã‚’è‡ªç„¶è¨€èªã§å…¥åŠ›ã—ã¦ãã ã•ã„",
        placeholder="ä¾‹: ä»Šæœˆã®å„ãƒ©ã‚¤ãƒ³ã®ç¨¼åƒç‡ã‚’æ¯”è¼ƒã—ã¦è¡¨ç¤º",
        height=100
    )
    
    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®å¯ç”¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    model_available = False
    if st.session_state.selected_model == 'azure_openai':
        model_available = st.session_state.openai_client is not None
    elif st.session_state.selected_model == 'gemma_2b':
        model_available = st.session_state.gemma_client is not None
    else:  # llama_elyza
        model_available = st.session_state.llama_client is not None
    
    if st.button("ã‚¯ã‚¨ãƒªå®Ÿè¡Œ", type="primary", disabled=not (st.session_state.db_connection and model_available)):
        if query_input:
            with st.spinner("SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­..."):
                try:
                    table_info = st.session_state.db_connection.get_table_schema()
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’å–å¾—
                    debug_mode = st.session_state.get('debug_mode', False)
                    
                    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦SQLç”Ÿæˆ
                    if st.session_state.selected_model == 'azure_openai':
                        ai_client = st.session_state.openai_client
                        model_name = "Azure OpenAI"
                    elif st.session_state.selected_model == 'gemma_2b':
                        ai_client = st.session_state.gemma_client
                        model_name = "Gemma-2-2b-jpn-it"
                    else:  # llama_elyza
                        ai_client = st.session_state.llama_client
                        model_name = "Llama-3-ELYZA-JP-8B"
                    
                    result = ai_client.generate_sql(
                        query_input, 
                        table_info,
                        debug=debug_mode
                    )
                    
                    if debug_mode:
                        sql_query, debug_info = result
                        
                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º
                        with st.expander("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=True):
                            st.subheader(f"{model_name}ã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
                            st.text_area("é€ä¿¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", debug_info["prompt"], height=300)
                            
                            st.subheader(f"{model_name}ã‹ã‚‰ã®ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
                            st.code(debug_info["raw_response"], language="text")
                            
                            col1, col2 = st.columns(2)
                            with col1:
                                st.metric("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«", debug_info["model"])
                            with col2:
                                if debug_info["tokens_used"]:
                                    st.metric("ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°", debug_info["tokens_used"])
                    else:
                        sql_query = result
                    
                    # SQLã‚¯ã‚¨ãƒªã‚’å…¨å¹…ã§è¡¨ç¤º
                    st.subheader("ç”Ÿæˆã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª")
                    st.code(sql_query, language="sql")
                    
                    with st.spinner("ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œä¸­..."):
                        df = st.session_state.db_connection.execute_query(sql_query)
                        
                        if df is not None and not df.empty:
                            # ã‚¯ã‚¨ãƒªçµæœã‚’å…¨å¹…ã§è¡¨ç¤º
                            st.subheader("ã‚¯ã‚¨ãƒªçµæœ")
                            st.dataframe(df, use_container_width=True)
                            
                            visualizer = DataVisualizer()
                            chart_type = ai_client.suggest_visualization(
                                query_input, df
                            )
                            
                            # ã‚°ãƒ©ãƒ•ã‚’å…¨å¹…ã§è¡¨ç¤º
                            st.subheader("ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–")
                            fig = visualizer.create_chart(df, chart_type, query_input)
                            if fig:
                                st.plotly_chart(fig, use_container_width=True)
                        else:
                            st.warning("ã‚¯ã‚¨ãƒªçµæœãŒç©ºã§ã™")
                            
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        else:
            st.warning("å•ã„åˆã‚ã›å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    
    if st.session_state.db_connection:
        with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±"):
            try:
                schema_info = st.session_state.db_connection.get_table_schema()
                st.code(schema_info, language="text")
            except Exception as e:
                st.error(f"ã‚¹ã‚­ãƒ¼ãƒå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == "__main__":
    main()