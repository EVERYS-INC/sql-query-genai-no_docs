import os
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class LlamaClient:
    def __init__(self, force_cpu=False):
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        self.local_model_path = Path("models/llama-3-elyza-jp-8b")
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        self.online_model_name = "elyza/Llama-3-ELYZA-JP-8B"
        
        # GPU/CPUã®é¸æŠ
        if force_cpu:
            self.device = "cpu"
            print("ğŸ’» CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
        else:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            if self.device == "cuda":
                print(f"âš¡ GPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ (GPU: {torch.cuda.get_device_name(0)})")
            else:
                print("ğŸ’» GPUãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
        
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if self.local_model_path.exists() and (self.local_model_path / "config.json").exists():
                print(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {self.local_model_path}")
                model_path = str(self.local_model_path)
                local_mode = True
            else:
                raise ValueError(
                    f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.local_model_path.absolute()}\n"
                    "å…ˆã« 'python download_llama_model.py' ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
                )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
            print("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=local_mode
            )
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ã€é©åˆ‡ãªè¨­å®šã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
            if self.device == "cuda":
                print("GPUãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                print(f"åˆ©ç”¨å¯èƒ½ãªGPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
                
                # 4GB VRAMã®å ´åˆã¯8bité‡å­åŒ–ã‚’è©¦ã¿ã‚‹
                if torch.cuda.get_device_properties(0).total_memory < 8 * 1024**3:  # 8GBæœªæº€
                    print("âš ï¸ GPU VRAMãŒé™å®šçš„ãªãŸã‚ã€8bité‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¾ã™")
                    try:
                        # bitsandbytesã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®å ´åˆï¼‰
                        import bitsandbytes as bnb
                        
                        # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ—ã‚’ä½œæˆ
                        # é‡è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿GPUã«é…ç½®
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path,
                            load_in_8bit=True,
                            device_map="auto",
                            trust_remote_code=True,
                            local_files_only=local_mode,
                            llm_int8_enable_fp32_cpu_offload=True,  # CPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
                            max_memory={0: "3.5GB", "cpu": "20GB"}  # GPU/CPUãƒ¡ãƒ¢ãƒªåˆ†å‰²
                        )
                        print("âœ“ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆCPU/GPUãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰")
                    except (ImportError, Exception) as e:
                        print(f"âš ï¸ 8bité‡å­åŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        print("âš ï¸ FP16ãƒ¢ãƒ¼ãƒ‰ã§CPU/GPUåˆ†å‰²ã‚’è©¦ã¿ã¾ã™")
                        
                        # FP16ã§CPU/GPUåˆ†å‰²
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path,
                            torch_dtype=torch.float16,
                            device_map="auto",
                            trust_remote_code=True,
                            local_files_only=local_mode,
                            offload_folder="offload",  # ãƒ‡ã‚£ã‚¹ã‚¯ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ•ã‚©ãƒ«ãƒ€
                            offload_state_dict=True,  # çŠ¶æ…‹è¾æ›¸ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
                            max_memory={0: "3GB", "cpu": "16GB"}  # GPU/CPUåˆ†å‰²
                        )
                else:
                    # ååˆ†ãªVRAMãŒã‚ã‚‹å ´åˆ
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        trust_remote_code=True,
                        local_files_only=local_mode
                    )
            else:
                # CPUã®å ´åˆ
                print("CPUãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True,
                    local_files_only=local_mode
                )
                self.model = self.model.to(self.device)
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
            if self.device == "cuda":
                print(f"âœ“ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
                print(f"   GPUä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
                print(f"   GPUäºˆç´„ãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
            else:
                print(f"âœ“ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† (device: {self.device})")
                
        except Exception as e:
            raise ValueError(f"Llama-3-ELYZA-JP-8Bãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def generate_sql(self, natural_language_query, table_schema, debug=False):
        prompt = f"""
ã‚ãªãŸã¯è£½é€ æ¥­ã®å·¥å ´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç²¾é€šã—ãŸSQLå°‚é–€å®¶ã§ã™ã€‚
**é‡è¦: ã“ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯SQLiteã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚SQLiteå›ºæœ‰ã®æ§‹æ–‡ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚**

ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ :
{table_schema}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å•ã„åˆã‚ã›:
{natural_language_query}

ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š
1. å®Ÿè¡Œå¯èƒ½ãªæ­£ç¢ºãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„
2. èª¬æ˜ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„
3. æ—¥ä»˜ã¯'YYYY-MM-DD'å½¢å¼ã§æ‰±ã£ã¦ãã ã•ã„
4. é›†è¨ˆã™ã‚‹å ´åˆã¯é©åˆ‡ãªGROUP BYã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
5. çµæœã¯è¦‹ã‚„ã™ã„ã‚ˆã†ã«ORDER BYã§ä¸¦ã³æ›¿ãˆã¦ãã ã•ã„

SQLiteå›ºæœ‰ã®æ³¨æ„äº‹é …ï¼š
- DATE_TRUNCé–¢æ•°ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚ä»£ã‚ã‚Šã«strftime()ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- INTERVALæ¼”ç®—å­ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚ä»£ã‚ã‚Šã«date()é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- CURRENT_DATEã®ä»£ã‚ã‚Šã«date('now')ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- ä»Šæœˆã®ç¯„å›²: strftime('%Y-%m-01', 'now') ã‹ã‚‰ strftime('%Y-%m-01', 'now', '+1 month')
- ä»Šé€±ã®ç¯„å›²: date('now', 'weekday 0', '-7 days') ã‹ã‚‰ date('now', 'weekday 0')
- ä»Šæ—¥: date('now')

SQLã‚¯ã‚¨ãƒª:
"""
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºä¸­...")
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
            
            # å…¥åŠ›ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚æ®µéšçš„ã«ï¼‰
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            print(f"å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(inputs['input_ids'][0])}")
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«å¿œã˜ãŸç”Ÿæˆè¨­å®š
            if self.device == "cuda":
                generation_config = {
                    "max_new_tokens": 500,
                    "temperature": 0.1,
                    "do_sample": True,
                    "top_p": 0.9,
                    "top_k": 50,
                    "repetition_penalty": 1.1,
                    "pad_token_id": self.tokenizer.pad_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                }
                print("âš¡ GPUã§SQLç”Ÿæˆä¸­...")
            else:
                generation_config = {
                    "max_new_tokens": 300,  # CPUç”¨ã«ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’èª¿æ•´
                    "temperature": 0.1,
                    "do_sample": False,  # æ±ºå®šçš„ãªç”Ÿæˆï¼ˆé«˜é€ŸåŒ–ï¼‰
                    "pad_token_id": self.tokenizer.pad_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                }
                print("ğŸ’» CPUã§SQLç”Ÿæˆä¸­... (ç´„30-60ç§’ã‹ã‹ã‚Šã¾ã™)")
            
            # SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
            import time
            start_time = time.time()
            
            with torch.no_grad():
                if self.device == "cpu":
                    # CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æœ€é©åŒ–
                    torch.set_num_threads(8)  # CPUã‚³ã‚¢æ•°ã«å¿œã˜ã¦èª¿æ•´
                
                # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                    print(f"ç”Ÿæˆé–‹å§‹å‰ã®GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆï¼ˆé€²æ—ã‚’è¡¨ç¤ºï¼‰
                print("SQLç”Ÿæˆä¸­...", end="", flush=True)
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã®ãŸã‚ã«å°ã•ãªãƒãƒƒãƒã§ç”Ÿæˆ
                try:
                    outputs = self.model.generate(
                        **inputs,
                        **generation_config
                    )
                    print(" å®Œäº†ï¼")
                except torch.cuda.OutOfMemoryError:
                    print("\nâš ï¸ GPUãƒ¡ãƒ¢ãƒªä¸è¶³ï¼CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦ã¿ã¾ã™...")
                    # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¦CPUã§å†è©¦è¡Œ
                    torch.cuda.empty_cache()
                    self.model = self.model.to("cpu")
                    inputs = {k: v.to("cpu") for k, v in inputs.items()}
                    outputs = self.model.generate(**inputs, **generation_config)
            
            elapsed_time = time.time() - start_time
            print(f"âœ… SQLç”Ÿæˆå®Œäº† (å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’)")
            
            # GPUãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’è¡¨ç¤º
            if self.device == "cuda":
                print(f"   æœ€çµ‚GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
                torch.cuda.empty_cache()  # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’å‰Šé™¤ã—ã¦ã€ç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿ã‚’å–å¾—
            sql_query = response[len(prompt):].strip()
            
            # SQLã‚¯ã‚¨ãƒªã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
            
            # æœ€åˆã®SELECTæ–‡ã‚’æŠ½å‡º
            lines = sql_query.split('\n')
            sql_lines = []
            in_sql = False
            for line in lines:
                if 'SELECT' in line.upper() or in_sql:
                    in_sql = True
                    sql_lines.append(line)
                    if ';' in line:
                        break
            
            sql_query = '\n'.join(sql_lines).strip()
            
            if debug:
                debug_info = {
                    "prompt": prompt,
                    "raw_response": response,
                    "cleaned_sql": sql_query,
                    "model": self.model_name,
                    "device": self.device,
                    "tokens_used": len(inputs['input_ids'][0]) + len(outputs[0])
                }
                return sql_query, debug_info
            
            return sql_query
            
        except Exception as e:
            raise Exception(f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def suggest_visualization(self, query, dataframe):
        columns = list(dataframe.columns)
        dtypes = dataframe.dtypes.to_dict()
        sample_data = dataframe.head(5).to_dict('records')
        
        prompt = f"""
ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªçµæœã«å¯¾ã—ã¦ã€æœ€é©ãªå¯è¦–åŒ–æ–¹æ³•ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚

å…ƒã®ã‚¯ã‚¨ãƒª: {query}
ã‚«ãƒ©ãƒ : {columns}
ãƒ‡ãƒ¼ã‚¿å‹: {json.dumps({k: str(v) for k, v in dtypes.items()}, ensure_ascii=False)}
ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: {json.dumps(sample_data, ensure_ascii=False, default=str)}

ä»¥ä¸‹ã‹ã‚‰1ã¤ã ã‘é¸ã‚“ã§ãã ã•ã„ï¼š
- line: æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚„é€£ç¶šçš„ãªå¤‰åŒ–
- bar: ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®æ¯”è¼ƒ
- scatter: 2å¤‰æ•°ã®ç›¸é–¢é–¢ä¿‚
- pie: æ§‹æˆæ¯”ã®è¡¨ç¤º
- heatmap: å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
- table: è¡¨å½¢å¼ã§ã®è¡¨ç¤º

å›ç­”ã¯ã€é¸æŠè‚¢ã®ä¸­ã‹ã‚‰1ã¤ã®å˜èªã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
å›ç­”: """
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            generation_config = {
                "max_new_tokens": 10,
                "temperature": 0.1,
                "do_sample": True,
                "top_p": 0.9,
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_config)
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            chart_type = response[len(prompt):].strip().lower()
            
            # æœ€åˆã®å˜èªã®ã¿ã‚’å–å¾—
            chart_type = chart_type.split()[0] if chart_type.split() else "bar"
            
            valid_types = ["line", "bar", "scatter", "pie", "heatmap", "table"]
            
            if chart_type not in valid_types:
                return "bar"
            
            return chart_type
            
        except Exception as e:
            return "bar"