"""
Gemma-2-2b-jpn-it Client - è»½é‡ã§é«˜é€Ÿãªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«
4GB VRAMã§ã‚‚å¿«é©ã«å‹•ä½œ
"""

import os
import json
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import time
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class GemmaClient:
    def __init__(self, force_cpu=False):
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        self.local_model_path = Path("models/gemma-2-2b-jpn-it")
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        self.online_model_name = "google/gemma-2-2b-jpn-it"
        
        # GPU/CPUã®é¸æŠ
        if force_cpu:
            self.device = "cpu"
            print("ğŸ’» CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
        else:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            if self.device == "cuda":
                gpu_name = torch.cuda.get_device_name(0)
                vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"âš¡ GPUæ¤œå‡º: {gpu_name} ({vram:.1f}GB)")
                print("âœ¨ Gemma-2-2bã¯è»½é‡ãªãŸã‚ã€4GB VRAMã§ã‚‚å¿«é©ã«å‹•ä½œã—ã¾ã™")
            else:
                print("ğŸ’» GPUãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")
        
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if self.local_model_path.exists() and (self.local_model_path / "config.json").exists():
                print(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {self.local_model_path}")
                model_path = str(self.local_model_path)
                local_mode = True
                hf_token = None  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ãƒˆãƒ¼ã‚¯ãƒ³ä¸è¦
            else:
                # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
                print(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã¾ã™...")
                
                # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰HF_TOKENã‚’å–å¾—
                hf_token = os.environ.get("HF_TOKEN", None)
                if hf_token:
                    print("âœ“ Hugging Faceãƒˆãƒ¼ã‚¯ãƒ³ã‚’.envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ã¾ã—ãŸ")
                    model_path = self.online_model_name
                    local_mode = False
                else:
                    raise ValueError(
                        f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.local_model_path.absolute()}\n"
                        "ã¾ãŸã€HF_TOKENãŒ.envãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n"
                        "ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š\n"
                        "1. 'python download_gemma_model.py' ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n"
                        "2. .envãƒ•ã‚¡ã‚¤ãƒ«ã«HF_TOKEN=your-token-hereã‚’è¿½åŠ "
                    )
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
            print("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            tokenizer_kwargs = {
                "trust_remote_code": True,
                "local_files_only": local_mode
            }
            if not local_mode and hf_token:
                tokenizer_kwargs["token"] = hf_token
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                **tokenizer_kwargs
            )
            
            # ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®š
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
            if self.device == "cuda":
                print("GPUãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                
                # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ç”¨ã®å…±é€škwargs
                model_kwargs = {
                    "trust_remote_code": True,
                    "local_files_only": local_mode
                }
                if not local_mode and hf_token:
                    model_kwargs["token"] = hf_token
                
                # 4GB VRAMã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«è¨­å®š
                if torch.cuda.get_device_properties(0).total_memory < 6 * 1024**3:  # 6GBæœªæº€
                    print("âš ï¸ VRAMå®¹é‡ãŒé™å®šçš„ãªãŸã‚ã€8bité‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¾ã™")
                    try:
                        from transformers import BitsAndBytesConfig
                        quantization_config = BitsAndBytesConfig(
                            load_in_8bit=True,
                            llm_int8_enable_fp32_cpu_offload=True
                        )
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path,
                            quantization_config=quantization_config,
                            device_map="auto",
                            **model_kwargs
                        )
                        print("âœ“ 8bité‡å­åŒ–ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
                    except (ImportError, Exception) as e:
                        print(f"âš ï¸ 8bité‡å­åŒ–ã§ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")
                        print("é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ã¾ã™...")
                        model_kwargs["torch_dtype"] = torch.float16
                        model_kwargs["low_cpu_mem_usage"] = False  # meta deviceã‚¨ãƒ©ãƒ¼ã‚’å›é¿
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_path,
                            **model_kwargs
                        )
                        self.model = self.model.to(self.device)
                else:
                    # ååˆ†ãªVRAMãŒã‚ã‚‹å ´åˆ
                    model_kwargs["torch_dtype"] = torch.float16
                    model_kwargs["low_cpu_mem_usage"] = False  # meta deviceã‚¨ãƒ©ãƒ¼ã‚’å›é¿
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        **model_kwargs
                    )
                    self.model = self.model.to(self.device)
                
                print(f"âœ“ GPUãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
                print(f"   GPUä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            else:
                # CPUã®å ´åˆ
                print("CPUãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                cpu_kwargs = {
                    "torch_dtype": torch.float32,
                    "low_cpu_mem_usage": True,
                    "trust_remote_code": True,
                    "local_files_only": local_mode
                }
                if not local_mode and hf_token:
                    cpu_kwargs["token"] = hf_token
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    **cpu_kwargs
                )
                self.model = self.model.to(self.device)
                print("âœ“ CPUãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
                
        except Exception as e:
            raise ValueError(f"Gemma-2-2b-jpn-itãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def generate_sql(self, natural_language_query, table_schema, debug=False):
        """SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ"""
        
        # Gemmaç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        prompt = f"""<bos><start_of_turn>user
SQLiteã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®è³ªå•ã‚’SQLã‚¯ã‚¨ãƒªã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ :
{table_schema}

è³ªå•: {natural_language_query}

æ³¨æ„:
- SQLiteã®æ—¥ä»˜é–¢æ•°ã‚’ä½¿ç”¨
- ä»Šæœˆ: date('now', 'start of month') ã‹ã‚‰ date('now')
- å¿…è¦ãªJOINã®ã¿ä½¿ç”¨
- SELECTæ–‡ã‹ã‚‰å§‹ã¾ã‚‹SQLã‚¯ã‚¨ãƒªã‚’è¿”ã™<end_of_turn>
<start_of_turn>model
SELECT"""
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            print("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºä¸­...")
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            )
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            if self.device == "cuda":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            print(f"å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(inputs['input_ids'][0])}")
            
            # ç”Ÿæˆè¨­å®šï¼ˆã‚ˆã‚Šå®‰å®šã—ãŸå‡ºåŠ›ã®ãŸã‚èª¿æ•´ï¼‰
            generation_config = {
                "max_new_tokens": 300,  # SQLã‚¯ã‚¨ãƒªç”¨ã«å¢—åŠ 
                "temperature": 0.05,     # ã‚ˆã‚Šæ±ºå®šçš„ãªå‡ºåŠ›
                "do_sample": True,
                "top_p": 0.9,           # ã‚ˆã‚Šä¿å®ˆçš„ãªé¸æŠ
                "top_k": 30,            # ãƒˆãƒƒãƒ—å€™è£œã‚’çµã‚‹
                "repetition_penalty": 1.05  # è»½ã„ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶
            }
            
            if self.device == "cuda":
                print("âš¡ GPUé«˜é€Ÿç”Ÿæˆä¸­...")
            else:
                print("ğŸ’» CPUç”Ÿæˆä¸­ï¼ˆ5-15ç§’ï¼‰...")
            
            start_time = time.time()
            
            with torch.no_grad():
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                
                outputs = self.model.generate(
                    **inputs,
                    **generation_config,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            elapsed_time = time.time() - start_time
            print(f"âœ… SQLç”Ÿæˆå®Œäº† (å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’)")
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ¢ãƒ‡ãƒ«ã®å›ç­”éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
            # "SELECT"ã§çµ‚ã‚ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è€ƒæ…®
            if "SELECT" in response:
                # æœ€å¾Œã®SELECTã®ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹
                select_positions = [i for i in range(len(response)) if response[i:].startswith("SELECT")]
                if select_positions:
                    # æœ€ã‚‚é©åˆ‡ãªSELECTä½ç½®ã‚’é¸ã¶ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¾Œã®ã‚‚ã®ï¼‰
                    prompt_select_count = prompt.count("SELECT")
                    if len(select_positions) > prompt_select_count:
                        sql_query = "SELECT" + response[select_positions[prompt_select_count] + 6:]
                    else:
                        sql_query = response[select_positions[-1]:]
                else:
                    sql_query = response.strip()
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                if "model" in response:
                    model_idx = response.rfind("model")
                    if model_idx != -1:
                        sql_query = response[model_idx + 5:].strip()
                    else:
                        sql_query = response.strip()
                else:
                    sql_query = response[len(prompt):].strip()
            
            
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®å‡¦ç†ã‚’æ”¹å–„
            if "```" in sql_query:
                # ```sql ã¾ãŸã¯ ``` ã®å‡¦ç†
                lines = sql_query.split('\n')
                in_code_block = False
                sql_lines = []
                
                for line in lines:
                    if line.strip().startswith("```"):
                        if not in_code_block:
                            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹
                            in_code_block = True
                        else:
                            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†
                            break
                    elif in_code_block:
                        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                        sql_lines.append(line)
                
                if sql_lines:
                    sql_query = '\n'.join(sql_lines).strip()
                else:
                    # å¤ã„æ–¹æ³•ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if "```sql" in sql_query.lower():
                        start_idx = sql_query.lower().find("```sql") + 6
                    else:
                        start_idx = sql_query.find("```") + 3
                        # æ”¹è¡Œã¾ã§ã‚¹ã‚­ãƒƒãƒ—
                        newline_idx = sql_query.find("\n", start_idx)
                        if newline_idx != -1 and newline_idx - start_idx < 10:
                            start_idx = newline_idx + 1
                    
                    end_idx = sql_query.find("```", start_idx)
                    if end_idx != -1:
                        sql_query = sql_query[start_idx:end_idx].strip()
                    else:
                        sql_query = sql_query[start_idx:].strip()
            
            # SQLã‚¯ã‚¨ãƒªãŒé©åˆ‡ã«å§‹ã¾ã£ã¦ã„ã‚‹ã‹ç¢ºèª
            sql_upper = sql_query.upper()
            if not sql_upper.startswith(("SELECT", "INSERT", "UPDATE", "DELETE", "WITH")):
                # SELECTæ–‡ã‚’æ¢ã™
                if "SELECT" in sql_upper:
                    sql_start = sql_upper.find("SELECT")
                    sql_query = sql_query[sql_start:]
            
            # ã‚»ãƒŸã‚³ãƒ­ãƒ³ã§çµ‚ç«¯
            if ";" in sql_query:
                sql_query = sql_query[:sql_query.find(";")+1]
            
            # æœ€çµ‚çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            sql_query = sql_query.strip()
            
            if debug:
                debug_info = {
                    "prompt": prompt,
                    "raw_response": response,
                    "cleaned_sql": sql_query,
                    "model": "Gemma-2-2b-jpn-it",
                    "device": self.device,
                    "processing_time": f"{elapsed_time:.1f}ç§’",
                    "tokens_used": len(inputs['input_ids'][0]) + len(outputs[0])
                }
                return sql_query, debug_info
            
            return sql_query
            
        except Exception as e:
            raise Exception(f"SQLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def suggest_visualization(self, query, dataframe):
        """å¯è¦–åŒ–ã‚¿ã‚¤ãƒ—ã‚’ææ¡ˆ"""
        columns = list(dataframe.columns)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ææ¡ˆï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
        if any("date" in col.lower() or "æ—¥" in col for col in columns):
            return "line"
        elif len(columns) == 2 and len(dataframe) <= 10:
            return "pie"
        elif any("rate" in col.lower() or "ç‡" in col for col in columns):
            return "bar"
        else:
            return "bar"